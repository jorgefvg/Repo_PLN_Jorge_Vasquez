# Desafios practicos (curso de PLN)

Este repositorio contiene el desarrollo de los **cuatro desaf√≠os pr√°cticos** de la asignatura **Procesamiento del Lenguaje Natural (PLN)**, realizados por **Jorge V√°squez**.  

---

## üìÇ Estructura del repositorio

El repositorio est√° organizado en cuatro carpetas principales, con una notebook por cada desaf√≠o.
Cada notebook contiene el c√≥digo, resultados, an√°lisis y conclusiones correspondientes a su desaf√≠o.

---

## Desaf√≠o 1 ‚Äî Vectorizaci√≥n y Clasificaci√≥n de Documentos

**Objetivo:** Analizar la representaci√≥n vectorial de documentos y su uso en tareas de similaridad y clasificaci√≥n.

**Actividades realizadas:**
1. **Vectorizaci√≥n de documentos:** Selecci√≥n de 5 documentos al azar y c√°lculo de similaridad con el resto para analizar coherencia sem√°ntica.
2. **Clasificaci√≥n por prototipos (Zero-Shot):** Asignaci√≥n de clases a documentos de test bas√°ndose en la similaridad con los de entrenamiento.
3. **Modelos de Na√Øve Bayes:** Entrenamiento y ajuste de modelos *MultinomialNB* y *ComplementNB* para maximizar el *F1-score macro*.
4. **Vectorizaci√≥n de palabras:** Transposici√≥n de la matriz documento-t√©rmino para estudiar la similaridad entre palabras seleccionadas.

---

## Desaf√≠o 2 ‚Äî Embeddings de Palabras con Gensim

**Objetivo:** Crear representaciones vectoriales (embeddings) propias a partir de corpus textuales.

**Actividades realizadas:**
1. Entrenamiento de **modelos Word2Vec** con **Gensim**, utilizando letras de canciones como corpus.
2. Exploraci√≥n de t√©rminos de inter√©s y an√°lisis de similitud sem√°ntica entre palabras.
3. Visualizaci√≥n de los embeddings en el espacio vectorial.
4. Confirmar que los embeddings entrenados con Gensim reflejan el contexto espec√≠fico en el que cada palabra aparece en las canciones, diferente al sentido general que tendr√≠an en otro corpus (ej. noticias, Wikipedia).

---

## Desaf√≠o 3 ‚Äî Modelos de lenguaje y generaci√≥n de secuencias.

**Objetivo:** Implementar un **modelo de lenguaje con tokenizaci√≥n por caracteres,** basado en redes neuronales recurrentes (RNN, LSTM, GRU).

**Actividades realizadas:**
1. Selecci√≥n y **preprocesamiento de un corpus** de texto para tokenizaci√≥n por caracteres.
2. Creaci√≥n del dataset y divisi√≥n en entrenamiento y validaci√≥n.
3. Implementaci√≥n de modelos basados en **SimpleRNN**, **LSTM** y **GRU**, entrenados con *rmsprop*.
4. Generaci√≥n de texto utilizando estrategias de:
   - **Greedy Search**
   - **Beam Search** (determin√≠stico y estoc√°stico)
5. An√°lisis del efecto de la **temperatura** en la generaci√≥n de texto con beam estocastico.

---

## Desaf√≠o 4 ‚Äî Traductor con LSTM (modelos Seq2Seq)

**Objetivo:** Implementar un **modelo traductor de ingl√©s a espa√±ol** basado en redes LSTM con arquitectura **encoder‚Äìdecoder**.

**Actividades realizadas:**
1. **Replicar y mejorar** un modelo de traducci√≥n de ingl√©s a espa√±ol seq2seq (encoder-decoder) utilizando **PyTorch**.
2. Entrenamiento extendido con m√°s datos y secuencias m√°s largas.
3. Estudio del impacto del n√∫mero de neuronas en las capas recurrentes.
4. Presentaci√≥n de **5 ejemplos de traducciones generadas**.
5. (Opcional) Experimentos con **embeddings preentrenados** (Glove/FastText) y estrategias de generaci√≥n alternativas (como muestreo aleatorio).

---
